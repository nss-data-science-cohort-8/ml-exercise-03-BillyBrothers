{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "297e4ea8-a64f-4aee-a147-d6b56ad6b7f2",
   "metadata": {},
   "source": [
    "## Machine Learning Exercise 3: Bias and Variance\n",
    "\n",
    "**Bias** refers to the error introduced by approximating a complex real-world problem with a simplified model, while **variance** refers to the model's sensitivity to fluctuations in the training data. A linear regression model has high bias and low variance; it makes strong assumptions about the data (linearity) but is stable across different datasets. If these strong assumptions are not correct, there will be places where it systematically overestimates or underestimates. In contrast, a decision tree model has low bias and high variance; it can capture complex patterns but is prone to overfitting, especially if deep and unpruned. This means that it can start to memorize the training data rather than capturing patterns that generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299c7a7b-7205-4eaf-b7e6-427864d88a8d",
   "metadata": {},
   "source": [
    "### 1. Fit a linear regression model to the housing data, using sqft_living to predict price. Check the mean squared error on the training data and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bbc4e106-d6e0-4c37-97d8-b0f32b2c85d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, RidgeCV, LassoCV\n",
    "from sklearn.metrics import confusion_matrix, RocCurveDisplay, classification_report, classification_report, mean_squared_error, root_mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52d5a3d6-1382-4ee3-8e7e-bc1718f7eac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21608</th>\n",
       "      <td>263000018</td>\n",
       "      <td>20140521T000000</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1530</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6993</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>6600060120</td>\n",
       "      <td>20150223T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2310</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5107</td>\n",
       "      <td>-122.362</td>\n",
       "      <td>1830</td>\n",
       "      <td>7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>1523300141</td>\n",
       "      <td>20140623T000000</td>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5944</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>291310100</td>\n",
       "      <td>20150116T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>98027</td>\n",
       "      <td>47.5345</td>\n",
       "      <td>-122.069</td>\n",
       "      <td>1410</td>\n",
       "      <td>1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>1523300157</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5941</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21613 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             date     price  bedrooms  bathrooms  \\\n",
       "0      7129300520  20141013T000000  221900.0         3       1.00   \n",
       "1      6414100192  20141209T000000  538000.0         3       2.25   \n",
       "2      5631500400  20150225T000000  180000.0         2       1.00   \n",
       "3      2487200875  20141209T000000  604000.0         4       3.00   \n",
       "4      1954400510  20150218T000000  510000.0         3       2.00   \n",
       "...           ...              ...       ...       ...        ...   \n",
       "21608   263000018  20140521T000000  360000.0         3       2.50   \n",
       "21609  6600060120  20150223T000000  400000.0         4       2.50   \n",
       "21610  1523300141  20140623T000000  402101.0         2       0.75   \n",
       "21611   291310100  20150116T000000  400000.0         3       2.50   \n",
       "21612  1523300157  20141015T000000  325000.0         2       0.75   \n",
       "\n",
       "       sqft_living  sqft_lot  floors  waterfront  view  ...  grade  \\\n",
       "0             1180      5650     1.0           0     0  ...      7   \n",
       "1             2570      7242     2.0           0     0  ...      7   \n",
       "2              770     10000     1.0           0     0  ...      6   \n",
       "3             1960      5000     1.0           0     0  ...      7   \n",
       "4             1680      8080     1.0           0     0  ...      8   \n",
       "...            ...       ...     ...         ...   ...  ...    ...   \n",
       "21608         1530      1131     3.0           0     0  ...      8   \n",
       "21609         2310      5813     2.0           0     0  ...      8   \n",
       "21610         1020      1350     2.0           0     0  ...      7   \n",
       "21611         1600      2388     2.0           0     0  ...      8   \n",
       "21612         1020      1076     2.0           0     0  ...      7   \n",
       "\n",
       "       sqft_above  sqft_basement  yr_built  yr_renovated  zipcode      lat  \\\n",
       "0            1180              0      1955             0    98178  47.5112   \n",
       "1            2170            400      1951          1991    98125  47.7210   \n",
       "2             770              0      1933             0    98028  47.7379   \n",
       "3            1050            910      1965             0    98136  47.5208   \n",
       "4            1680              0      1987             0    98074  47.6168   \n",
       "...           ...            ...       ...           ...      ...      ...   \n",
       "21608        1530              0      2009             0    98103  47.6993   \n",
       "21609        2310              0      2014             0    98146  47.5107   \n",
       "21610        1020              0      2009             0    98144  47.5944   \n",
       "21611        1600              0      2004             0    98027  47.5345   \n",
       "21612        1020              0      2008             0    98144  47.5941   \n",
       "\n",
       "          long  sqft_living15  sqft_lot15  \n",
       "0     -122.257           1340        5650  \n",
       "1     -122.319           1690        7639  \n",
       "2     -122.233           2720        8062  \n",
       "3     -122.393           1360        5000  \n",
       "4     -122.045           1800        7503  \n",
       "...        ...            ...         ...  \n",
       "21608 -122.346           1530        1509  \n",
       "21609 -122.362           1830        7200  \n",
       "21610 -122.299           1020        2007  \n",
       "21611 -122.069           1410        1287  \n",
       "21612 -122.299           1020        1357  \n",
       "\n",
       "[21613 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_sales_df = pd.read_csv(\"data/kc_house_data.csv\")\n",
    "house_sales_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30f343fb-2847-41f6-bb8a-5683fe8940d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse training data 65713942542.233665:\n"
     ]
    }
   ],
   "source": [
    "X = house_sales_df[[\"sqft_living\"]]\n",
    "y = house_sales_df[\"price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_train)\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "print(\"mse training data {}:\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5cfe88-5bd2-4a04-b162-37aaa52e2b68",
   "metadata": {},
   "source": [
    "**Predictions based on X_train tell you how well your model fits the data it has already seen during training.**\n",
    "\n",
    "**A model that performs very well on X_train (with low errors, like MSE) could indicate that it has successfully learned patterns in the training data. However, if the accuracy is too high, it might suggest overfitting, where the model memorizes the training data instead of learning to generalize.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0683e822-47c6-4f4b-9b92-802ef4d15fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse testing data 74509993356.49603:\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"mse testing data {}:\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374a4af1-24e3-4f50-8c02-16bf161be8ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Predictions based on X_test allow you to assess how well your model generalizes to unseen dataâ€”real-world performance.**\n",
    "\n",
    "**If the performance on X_test is significantly worse than on X_train, it indicates overfittingâ€”the model struggles to generalize beyond the training data.**\n",
    "\n",
    "**Conversely, if the error is high for both X_train and X_test, the model might be underfittingâ€”it doesn't capture the underlying patterns effectively.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e3e476-bcfb-4f89-8c74-193098f13896",
   "metadata": {},
   "source": [
    "**Conclusion: The difference between the errors is notable but not excessively large. Typically, overfitting is characterized by a very low training error combined with a significantly higher testing error, showing that the model fits the training data well but struggles with unseen data.\n",
    "Here, while the testing error is slightly higher, the gap between the training and testing MSE isn't drastic enough to immediately conclude overfitting. Instead, it suggests potential underfitting or general model inefficiency, as both errors are quite high.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e4963-5506-45a4-b264-27434717bde5",
   "metadata": {},
   "source": [
    "### 2. Repeat this but with a [DecisionTreeRegresor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html). Again check the mean squared error on the training data and the test data. How does what you see differ from the linear regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa603f20-673f-4e0f-b510-32fa012a922f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse testing data 79044952597.92511:\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)\n",
    "tree_test = DecisionTreeRegressor(random_state = 42)\n",
    "tree_test.fit(X_train, y_train)\n",
    "tree_test.predict(X_test)\n",
    "y_pred_test = tree_test.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "print(\"mse testing data {}:\".format(mse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9108080-261e-425a-b6c5-98f5fda8f222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse training data 48410745078.45624:\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)\n",
    "tree_train = DecisionTreeRegressor(random_state = 42)\n",
    "tree_train.fit(X_train, y_train)\n",
    "y_pred_train = tree_train.predict(X_train)\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "print(\"mse training data {}:\".format(mse_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a6739-4318-4321-8380-e03b407863ce",
   "metadata": {},
   "source": [
    "**In comparison to the earlier Linear Regression model, this suggests a larger gap between the training and testing errors in the Decision Tree model. This larger gap is a red flag for potential overfitting. Decision Trees, being highly flexible models, tend to overfit the training data if not properly constrained.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f387a9-a6ba-43df-bc64-ab01fba43355",
   "metadata": {},
   "source": [
    "### One way of avoiding overfitting is by restricting the flexibility of the model. We can do this with a decision tree by restricting the number of splits that it can perform. \n",
    "\n",
    "### 3. Fit a DecisionTreeRegressor where you restrict the max_depth to 5. Again check the mean squared error on the training data and the test data. What do you notice now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b760f0df-f4c3-4f02-a0f7-a906fd97b821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse testing data 73264250436.88351:\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)\n",
    "tree = DecisionTreeRegressor(random_state = 42, max_depth = 5)\n",
    "tree.fit(X_train, y_train)\n",
    "tree.predict(X_test)\n",
    "y_pred = tree.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"mse testing data {}:\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a779171-2047-4d74-89b1-bd1f9742dc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse training data 56030892751.02288:\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)\n",
    "tree = DecisionTreeRegressor(random_state = 42, max_depth = 5)\n",
    "tree.fit(X_train, y_train)\n",
    "tree.predict(X_train)\n",
    "y_pred = tree.predict(X_train)\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "print(\"mse training data {}:\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f380ecc-0618-46b2-9d11-aae719c8d372",
   "metadata": {},
   "source": [
    "**Conclusion: By adding max_depth=5, ive constrained the Decision Tree to prevent it from becoming overly complex, which likely reduced overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6399b24e-4704-4ad8-a2bc-b9369b5308bd",
   "metadata": {},
   "source": [
    "\n",
    "**Root Node:**\n",
    "\n",
    "The starting point of the tree, where the data is initially split based on the feature (input variable) that minimizes the mean squared error (MSE) for regression tasks.\n",
    "\n",
    "\n",
    "**Internal Nodes** (Decision Nodes):\n",
    "\n",
    "These are points in the tree where further splitting occurs. Each node contains:\n",
    "\n",
    "A feature on which the split is based.\n",
    "\n",
    "A threshold value or condition.\n",
    "\n",
    "Statistical measures to determine the \"quality\" of the split, like the reduction in variance or mean squared error for regression.\n",
    "\n",
    "**Leaf Nodes** (Terminal Nodes):\n",
    "\n",
    "These are the endpoints of the tree and represent the final predictions.\n",
    "\n",
    "In regression, each leaf node contains the predicted target value, typically the mean or median of the target values for the subset of data that reaches that leaf.\n",
    "\n",
    "\n",
    "Root: sqft_living <= 3000?\n",
    "   â”œâ”€â”€ Yes: sqft_living <= 1500?\n",
    "   â”‚      â”œâ”€â”€ Yes: Price = $250,000\n",
    "   â”‚      â”œâ”€â”€ No: sqft_living <= 2000?\n",
    "   â”‚             â”œâ”€â”€ Yes: Price = $400,000\n",
    "   â”‚             â”œâ”€â”€ No: Price = $500,000\n",
    "   â”œâ”€â”€ No: sqft_living <= 4000?\n",
    "          â”œâ”€â”€ Yes: Price = $700,000\n",
    "          â”œâ”€â”€ No: sqft_living <= 5000?\n",
    "                 â”œâ”€â”€ Yes: Price = $850,000\n",
    "                 â”œâ”€â”€ No: Price = $1,000,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feab77d-f755-403c-bf99-1fa58f0337d0",
   "metadata": {},
   "source": [
    "### When working with machine learning models, we often have to balance bias and variance. This is called the [bias-variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff). One method of this is through [regularization](https://www.ibm.com/think/topics/regularization), where we restrict the complexity of the model, adding some bias but reducing the variance, which can lead to a lower mean squared error on the test set.\n",
    "\n",
    "### Lasso and ridge regression do this by adding a penalty term based on the size of the coefficients. Smaller coefficients means that the model has less flexibility. The neat thing about these types of models is that they determine how to allocate the coefficients automatically as part of the model fitting process, so we can start with a large set of potential predictors and allow the model fitting to determine which ones to focus on.\n",
    "\n",
    "### For the next part of the exercise, we'll see how we can add complexity to our model but control the complexity through regularization.\n",
    "\n",
    "### 4. So far, we've only been predicting based off of the square footage of living space. Fit a new linear regression model using all variables besides id, date, price, and zipcode. How well does this model perform on the test set compared to the model with just square footage of living space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27a85592-09e9-4ba4-9690-db7b0d10c6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse testing data 44093475276.572525:\n"
     ]
    }
   ],
   "source": [
    "X = house_sales_df.drop([\"id\", \"date\", \"price\", \"zipcode\"], axis=1)\n",
    "y = house_sales_df['price'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"mse testing data {}:\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c052d0-2d0b-4357-acce-c825864fd33c",
   "metadata": {},
   "source": [
    "**Conclusion: The MSE with multiple predictor's is significantly less than the MSE of the linear regression model with only the sq_ft for the predictor**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a8f612-7ee9-4722-bd4b-4512bb98a2d3",
   "metadata": {},
   "source": [
    "### 5. Try fitting a lasso and ridge model. Becuase lasso and ridge have penalty terms based on the size of the coefficients, and the size of the coefficients depends on the scale of the variable, you'll want to scale the features first so that they are on comparable scales. Create a [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) object where the first step is applying a [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and the second step is either a lasso or ridge model. Because these models have a hyperparameter controlling regularization strength, you'll want to use the [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) and [RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) models, which will select values for the regularization strength using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99809b54-75b2-4f72-9397-52ec3ddc4dc3",
   "metadata": {},
   "source": [
    "#### LassoCV and RidgeCV are specialized regression models in Scikit-learn that automatically select the best hyperparameter (ð›¼) for Lasso Regression and Ridge Regression, respectively, using cross-validation.\n",
    "    * Lasso CV uses L1 regularization, which penalizes the absolute values of coefficients. It encourages sparsity, meaning it can shrink some coefficients to zero, effectively performing feature selection.\n",
    "    * Ridge CV uses L2 regularization, which penalizes the square of coefficients. It shrinks coefficients but does not eliminate them, making it useful when all predictors are relevant, but multicollinearity exists (predictors are highly correlated).\n",
    "    * Both models simplify the hyperparameter tuning process because they have built-in cross-validation, so you donâ€™t need to use tools like GridSearchCV separately.\n",
    "#### StandardScaler \n",
    "    * Standardizing:  The combination of centering and scaling. Specifically, it transforms data such that each feature has a mean of 0 and a standard deviation of 1. This ensures features contribute equally to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f2bab8ba-be7e-4780-8b0b-a7d8f5446379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Ridge Regression score 0.6945638061327064:\n",
      "MSE Ridge Regression score 44094763932.52483:\n"
     ]
    }
   ],
   "source": [
    "X = house_sales_df.drop([\"id\", \"date\", \"price\", \"zipcode\"], axis=1)\n",
    "y = house_sales_df['price'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)\n",
    "steps = [(\"StandardScaling\", StandardScaler()), \n",
    "         (\"RidgeRegression\", RidgeCV())]\n",
    "pipeline = Pipeline(steps)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "r2 = pipeline.score(X_test, y_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"R^2 Ridge Regression score {}:\".format(r2))\n",
    "print(\"MSE Ridge Regression score {}:\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "72cb0ec6-c307-4595-9d74-755fca0cdd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Lasso Regression score 0.6943271474783299:\n",
      "Lasso MSE Regression score 44128929521.62934:\n"
     ]
    }
   ],
   "source": [
    "X = house_sales_df.drop([\"id\", \"date\", \"price\", \"zipcode\"], axis=1)\n",
    "y = house_sales_df['price'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)\n",
    "steps = [(\"StandardScaling\", StandardScaler()), \n",
    "         (\"LassoRegression\", LassoCV())]\n",
    "pipeline = Pipeline(steps)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "r2 = pipeline.score(X_test, y_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"R^2 Lasso Regression score {}:\".format(r2))\n",
    "print(\"Lasso MSE Regression score {}:\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d146927-32ea-4dd1-b096-694a96afe2e2",
   "metadata": {},
   "source": [
    "**Interpretation: model explains roughly 69% of the variance in the target variable. The slight edge for Ridge Regression might suggest it fits your data a bit better, potentially due to its L2 regularization handling multicollinearity more effectively than the L1 regularization of Lasso.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01e4ad-4e76-4709-992f-6847be49b48f",
   "metadata": {},
   "source": [
    "### You likely didn't see much difference between the regular linear regression model and the lasso or ridge model. Let's see what happens if we add more complexity through feature interactions. We can capture more complex relationships between the predictor variables and the target variable by multiplying the predictors together before fitting the model. For example, the interaction between sqft_living and bedrooms will let the model capture if the impact of square footage depends on the number of bedrooms.\n",
    "\n",
    "### 6. Add [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) to your pipeline after the standard scaler. Try using degree 2 features. How does this change the performance of the regular linear regression model, the lasso model, and the ridge model? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c3cb5a-bdf7-4a2d-9991-5edb4303b780",
   "metadata": {},
   "source": [
    "#### PolynomialFeatures is a powerful preprocessing tool in Scikit-learn that creates polynomial features from your existing input data (features). In essence, it generates a new feature matrix containing not only the original features but also their polynomial combinations and interactions up to a specified degree.\n",
    "    * Suppose you specify a polynomial degree of 2. PolynomialFeatures will create a new feature matrix containing:\n",
    "        * The original features\n",
    "        * The squared terms of each feature\n",
    "        * The interaction terms between pairs of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b9df9b0e-2024-4acd-898e-ae9ab5e61320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Linear Regression score 30926921716.856598:\n"
     ]
    }
   ],
   "source": [
    "X = house_sales_df.drop([\"id\", \"date\", \"price\", \"zipcode\"], axis=1)\n",
    "y = house_sales_df['price'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)\n",
    "\n",
    "steps = [(\"StandardScaling\", StandardScaler()),\n",
    "         (\"PolynomialFeatures\", PolynomialFeatures(degree=2)),\n",
    "         (\"LinearRegression\", LinearRegression())]\n",
    "pipeline = Pipeline(steps)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE Linear Regression score {}:\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4126979f-7e30-4827-8685-e04b3eba749a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Ridge Regression score 30933833753.383926:\n"
     ]
    }
   ],
   "source": [
    "X = house_sales_df.drop([\"id\", \"date\", \"price\", \"zipcode\"], axis=1)\n",
    "y = house_sales_df['price'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)\n",
    "\n",
    "steps = [(\"StandardScaling\", StandardScaler()),\n",
    "         (\"PolynomialFeatures\", PolynomialFeatures(degree=2)),\n",
    "         (\"RidgeRegression\", RidgeCV())]\n",
    "pipeline = Pipeline(steps)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE Ridge Regression score {}:\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b68a4edd-29c7-4cb0-9a67-613977435e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Lasso Regression score 30414023904.74287:\n"
     ]
    }
   ],
   "source": [
    "X = house_sales_df.drop([\"id\", \"date\", \"price\", \"zipcode\"], axis=1)\n",
    "y = house_sales_df['price'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)\n",
    "\n",
    "steps = [(\"StandardScaling\", StandardScaler()),\n",
    "         (\"PolynomialFeatures\", PolynomialFeatures(degree=2)),\n",
    "         (\"LassoRegression\", LassoCV())]\n",
    "pipeline = Pipeline(steps)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE Lasso Regression score {}:\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff102ce2-9caf-4c2f-a5f0-efbaa2f94039",
   "metadata": {},
   "source": [
    "**Interpretation: Adding in the PolynomialFeature function drove down the MSE a lot.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b07b81-5352-466f-92b5-a5e5914e64b3",
   "metadata": {},
   "source": [
    "### The lasso penalty tends to cause some coeffients to zero out, so it can be viewed as a method of automatic feature selection.\n",
    "### 7. Look at the set of coefficients for the lasso model. What percentage of the coefficients are zero? What are the largest non-zero coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6ddfb80b-0d12-4fb9-a7c9-41fef2387ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Lasso coefficients [ 0.00000000e+00 -0.00000000e+00  1.29631038e+04  9.30898037e+04\n",
      " -0.00000000e+00 -7.40640157e+03  0.00000000e+00  0.00000000e+00\n",
      "  2.55901284e+04  9.49584662e+04  3.48881548e+03  0.00000000e+00\n",
      " -1.89758098e+04  0.00000000e+00  6.91512974e+04 -2.23709363e+04\n",
      "  4.64911392e+04 -0.00000000e+00  1.34675819e+02 -3.42301923e+03\n",
      " -5.54586218e+03  0.00000000e+00  0.00000000e+00 -8.21509899e+02\n",
      " -0.00000000e+00  2.68731640e+02  4.54886920e+02 -0.00000000e+00\n",
      " -7.66847183e+02 -4.16932354e+02 -2.08144768e+03 -0.00000000e+00\n",
      "  5.53855599e+03  2.71218569e+02  1.37786566e+03 -0.00000000e+00\n",
      "  1.94692202e+04 -3.77436715e+03 -5.05144532e+03 -0.00000000e+00\n",
      "  2.70220671e+03 -1.27452732e+03  9.42420011e+02  7.47394194e+03\n",
      "  0.00000000e+00  3.62574286e+03 -4.04970904e+03  0.00000000e+00\n",
      " -1.84220237e+03 -6.94222056e+02  0.00000000e+00  4.36724451e+03\n",
      " -5.07516407e+03 -0.00000000e+00  6.50325965e+03  0.00000000e+00\n",
      "  0.00000000e+00  3.34707747e+04  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.12849069e+04  1.88427149e+04 -1.92974825e+04\n",
      "  0.00000000e+00 -0.00000000e+00 -3.64151338e+00  0.00000000e+00\n",
      "  1.05481548e+03  1.64917083e+03  0.00000000e+00  1.82774005e+03\n",
      " -0.00000000e+00 -0.00000000e+00  7.50998142e+03 -3.01402238e+03\n",
      " -2.77950863e+03  4.86254680e+03  0.00000000e+00  1.38982217e+03\n",
      "  0.00000000e+00 -7.17748731e+03 -0.00000000e+00  4.24076570e+03\n",
      " -0.00000000e+00 -4.13340615e+03 -0.00000000e+00  0.00000000e+00\n",
      " -5.41410094e+02  0.00000000e+00 -0.00000000e+00 -2.19323615e+03\n",
      "  0.00000000e+00  4.44911272e+03 -1.24430690e+03  1.43351210e+03\n",
      " -5.90085930e+03  8.01702457e+03  0.00000000e+00  7.39133793e+03\n",
      "  1.13592527e+03  1.03483293e+04  1.11287126e+04  3.45395611e+03\n",
      "  4.78931439e+02  5.52222147e+03  1.90564838e+03  5.82835092e+03\n",
      "  0.00000000e+00 -1.16391318e+03  6.69986100e+02 -4.88595101e+03\n",
      "  1.21118164e+03  8.46343571e+02 -0.00000000e+00 -1.65503550e+03\n",
      " -0.00000000e+00 -0.00000000e+00  0.00000000e+00  8.91210628e+02\n",
      " -1.84027167e+03 -3.31997947e+03 -9.40398072e+02  0.00000000e+00\n",
      "  1.32212070e+04 -5.26397475e+03 -0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -0.00000000e+00  2.95522228e+03  1.95939812e+04\n",
      " -2.16187768e+04 -1.74047561e+02 -6.38990989e+02  0.00000000e+00\n",
      "  6.84237785e+02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -3.12891438e+03 -0.00000000e+00 -0.00000000e+00 -1.32151366e+04\n",
      " -2.44709272e+03  2.22387716e+03  0.00000000e+00 -0.00000000e+00\n",
      "  7.17101323e+03 -0.00000000e+00  1.98399910e+04  1.34178964e+03\n",
      " -1.16413417e+04  1.38986201e+04 -3.27771757e+04  2.45260297e+03\n",
      "  3.34830745e+03 -1.60201027e+03  3.56115240e+03  6.44427138e+03\n",
      " -0.00000000e+00 -4.10425611e+04 -1.45047685e+04  2.87635097e+03\n",
      " -9.03345392e+03 -2.67713957e+03 -2.98477385e+03  3.72643823e+02\n",
      "  3.59010509e+03 -4.60464565e+03 -7.57699582e+02]:\n",
      "The number of coefficients less than zero 49:\n"
     ]
    }
   ],
   "source": [
    "X = house_sales_df.drop([\"id\", \"date\", \"price\", \"zipcode\"], axis=1)\n",
    "y = house_sales_df['price'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)\n",
    "\n",
    "steps = [(\"StandardScaling\", StandardScaler()),\n",
    "         (\"PolynomialFeatures\", PolynomialFeatures(degree=2)),\n",
    "         (\"LassoRegression\", LassoCV())]\n",
    "pipeline = Pipeline(steps)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "Lasso_model = pipeline.named_steps[\"LassoRegression\"]\n",
    "Lasso_coef = Lasso_model.coef_\n",
    "print(\"MSE Lasso coefficients {}:\".format(Lasso_coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "801ae17e-663d-4534-bb5c-aaef969ce578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341509.3047541333"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(Lasso_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "99e8a9dd-7a00-4b5a-9e7a-fd0f1e8ce5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of coefficients less than zero 0.00014348071726852986:\n"
     ]
    }
   ],
   "source": [
    "CoefLessThanZero = np.sum(Lasso_coef < 0) / np.sum(Lasso_coef)\n",
    "print(\"The number of coefficients less than zero {}:\".format(CoefLessThanZero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "eda505c3-01be-4641-983f-240f97d101c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.49584662e+04,  9.30898037e+04,  6.91512974e+04,  4.64911392e+04,\n",
       "        3.34707747e+04,  2.55901284e+04,  1.98399910e+04,  1.95939812e+04,\n",
       "        1.94692202e+04,  1.88427149e+04,  1.38986201e+04,  1.32212070e+04,\n",
       "        1.29631038e+04,  1.12849069e+04,  1.11287126e+04,  1.03483293e+04,\n",
       "        8.01702457e+03,  7.50998142e+03,  7.47394194e+03,  7.39133793e+03,\n",
       "        7.17101323e+03,  6.50325965e+03,  6.44427138e+03,  5.82835092e+03,\n",
       "        5.53855599e+03,  5.52222147e+03,  4.86254680e+03,  4.44911272e+03,\n",
       "        4.36724451e+03,  4.24076570e+03,  3.62574286e+03,  3.59010509e+03,\n",
       "        3.56115240e+03,  3.48881548e+03,  3.45395611e+03,  3.34830745e+03,\n",
       "        2.95522228e+03,  2.87635097e+03,  2.70220671e+03,  2.45260297e+03,\n",
       "        2.22387716e+03,  1.90564838e+03,  1.82774005e+03,  1.64917083e+03,\n",
       "        1.43351210e+03,  1.38982217e+03,  1.37786566e+03,  1.34178964e+03,\n",
       "        1.21118164e+03,  1.13592527e+03,  1.05481548e+03,  9.42420011e+02,\n",
       "        8.91210628e+02,  8.46343571e+02,  6.84237785e+02,  6.69986100e+02,\n",
       "        4.78931439e+02,  4.54886920e+02,  3.72643823e+02,  2.71218569e+02,\n",
       "        2.68731640e+02,  1.34675819e+02, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -3.64151338e+00, -1.74047561e+02,\n",
       "       -4.16932354e+02, -5.41410094e+02, -6.38990989e+02, -6.94222056e+02,\n",
       "       -7.57699582e+02, -7.66847183e+02, -8.21509899e+02, -9.40398072e+02,\n",
       "       -1.16391318e+03, -1.24430690e+03, -1.27452732e+03, -1.60201027e+03,\n",
       "       -1.65503550e+03, -1.84027167e+03, -1.84220237e+03, -2.08144768e+03,\n",
       "       -2.19323615e+03, -2.44709272e+03, -2.67713957e+03, -2.77950863e+03,\n",
       "       -2.98477385e+03, -3.01402238e+03, -3.12891438e+03, -3.31997947e+03,\n",
       "       -3.42301923e+03, -3.77436715e+03, -4.04970904e+03, -4.13340615e+03,\n",
       "       -4.60464565e+03, -4.88595101e+03, -5.05144532e+03, -5.07516407e+03,\n",
       "       -5.26397475e+03, -5.54586218e+03, -5.90085930e+03, -7.17748731e+03,\n",
       "       -7.40640157e+03, -9.03345392e+03, -1.16413417e+04, -1.32151366e+04,\n",
       "       -1.45047685e+04, -1.89758098e+04, -1.92974825e+04, -2.16187768e+04,\n",
       "       -2.23709363e+04, -3.27771757e+04, -4.10425611e+04])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(Lasso_coef)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff0f1d8-78fe-41f5-8c05-2832bf0e9e96",
   "metadata": {},
   "source": [
    "**Interpretation: In Lasso Regression, a coefficient of zero means that the corresponding feature is completely excluded from the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7938b3-a975-48cf-8e2d-4bb4e58f1dc2",
   "metadata": {},
   "source": [
    "### 8. A new hyperparameter that we have is the degree of the polynomial we're using. So that we're not overfitting to the test set, we need to use cross-validation to select this value. Set up a [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to try out polynomial degrees from 1 to 3 and to try LinearRegression, LassoCV, and RidgeCV models. Use 'neg_mean_squared_error' as the error_score. Which combination does it find does the best? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0e98ca70-4acb-447d-a2e9-25abf2944ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'model': LinearRegression(), 'polynomialfeatures__degree': 2}\n",
      "Best neg MSE score: -25869569633.99911\n",
      "Test Mean Squared Error: 30926921716.856598\n"
     ]
    }
   ],
   "source": [
    "X = house_sales_df.drop([\"id\", \"date\", \"price\", \"zipcode\"], axis=1)\n",
    "y = house_sales_df['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"standardscaling\", StandardScaler()),\n",
    "    (\"polynomialfeatures\", PolynomialFeatures()),\n",
    "    (\"model\", LinearRegression()) \n",
    "])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"polynomialfeatures__degree\": [1, 2, 3],\n",
    "    \"model\": [LinearRegression(), RidgeCV(), LassoCV(cv=5, max_iter=10000)] \n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_mean_squared_error\", \n",
    "    cv=5\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best combination of parameters and the corresponding score\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Use the best model to make predictions on the test set\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best neg MSE score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate the test set performance\n",
    "y_pred = best_estimator.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Test Mean Squared Error: {test_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895c432a-c2d3-43be-9fb5-ff1eaf1f8792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
